{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fb225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sim\n",
    "import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = \"Data/train_corpus.txt\"\n",
    "test_words = [x.strip().split(\",\") for x in open(\"Data/test_tokens.txt\", encoding='utf8')]\n",
    "test_values = [x.strip() for x in open(\"Data/test_values.txt\", encoding='utf8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da28c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = sim.Baseline() \n",
    "term_doc_model = sim.Term_document(corpus=train_texts)\n",
    "window_model = sim.Window(corpus=train_texts, context_size=1)\n",
    "word2vec_model = sim.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5d2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = [baseline_model.calc_sim(word_1=x[0], word_2=x[1]) for x in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65839cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_results = [term_doc_model.calc_sim(word_1=x[0], word_2=x[1]) for x in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d339d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_results = [window_model.calc_sim(word_1=x[0], word_2=x[1]) for x in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f5087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_results = [word2vec_model.calc_sim(word_1=x[0], word_2=x[1]) for x in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f569004c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Errors\n",
      "Baseline: 7.02184094094094\n",
      "Term-Doc: 22.831604704704702\n",
      "Window: 13.556392992992993\n",
      "Word2Vec: 6.417409509509505\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Errors\")\n",
    "print(f\"Baseline: {score.MSE(preds=baseline_results, golds=test_values)}\")\n",
    "print(f\"Term-Doc: {score.MSE(preds=term_doc_results, golds=test_values)}\")\n",
    "print(f\"Window: {score.MSE(preds=window_results, golds=test_values)}\")\n",
    "print(f\"Word2Vec: {score.MSE(preds=word2vec_results, golds=test_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e654a7a",
   "metadata": {},
   "source": [
    "The results are a bit surprising, showing that the baseline model of just guessing the middle value is one of the best tested.\n",
    "\n",
    "A few solutions that could work to fix this are:\n",
    "- A better tokenizer. The one being used is just a very basic one. Puncuaction could be causing issues, as well as text like URLs.\n",
    "- Some sort of weight system where more frequent words have less of an impact as frequent words have a higher similarity score compared to less frequent words.\n",
    "- Stop words (and filler words) could be removed as well to only use more 'complex' words but that could cause issues for words that should be considered similar to them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
